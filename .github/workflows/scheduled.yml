name: Scheduled Tests

on:
  schedule:
    # Run every Monday at 00:00 UTC
    - cron: "0 0 * * 1"
  workflow_dispatch:

jobs:
  test-all-python-versions:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12", "3.13"]
        os: [ubuntu-latest, macos-latest, windows-latest]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Run tests
        run: |
          pytest -v

      - name: Verify zero dependencies
        run: |
          pytest tests/test_stdlib_only.py -v

  test-minimum-python-version:
    name: Test Minimum Python Version (3.10)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: "3.10.0"  # Exact minimum version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Run full test suite
        run: |
          pytest -v --cov=ascii_guard

  test-development-python:
    name: Test Python Development Version
    runs-on: ubuntu-latest
    continue-on-error: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.13 (latest available)
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          allow-prereleases: true

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Run tests
        run: |
          pytest -v

  dependency-audit:
    name: Dependency Audit
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install safety

      - name: Run safety check on dev dependencies
        run: |
          safety check --json || true

      - name: Check for outdated dependencies
        run: |
          pip list --outdated

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Create large test file
        run: |
          cat > large_test.md << 'EOF'
          # Large Test File

          This file contains many ASCII boxes for performance testing.

          EOF

          # Create 100 boxes
          for i in {1..100}; do
            cat >> large_test.md << 'EOF'

          ## Box $i

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Performance test   â”‚
          â”‚ Box number: $i     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          EOF
          done

      - name: Benchmark detection
        run: |
          time python -m ascii_guard.cli lint large_test.md

      - name: Benchmark fixing
        run: |
          time python -m ascii_guard.cli fix large_test.md --dry-run

  notify-failures:
    name: Notify on Failure
    runs-on: ubuntu-latest
    permissions:
      issues: write
    needs:
      [
        test-all-python-versions,
        test-minimum-python-version,
        dependency-audit,
        performance-benchmark,
      ]
    if: failure()

    steps:
      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          script: |
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ğŸš¨ Scheduled CI Tests Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Scheduled Tests Failed\n\n**Date:** ${new Date().toISOString()}\n\n**Workflow Run:** ${runUrl}\n\nPlease check the workflow run for details and fix any issues.`,
              labels: ['ci', 'scheduled-test-failure']
            });
