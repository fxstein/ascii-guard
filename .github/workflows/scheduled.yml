name: Scheduled Tests

on:
  schedule:
    # Run every Monday at 00:00 UTC
    - cron: "0 0 * * 1"
  workflow_dispatch:

jobs:
  test-multi-platform:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Run tests
        run: |
          pytest -v

      - name: Verify zero dependencies
        run: |
          pytest tests/test_stdlib_only.py -v

  dependency-audit:
    name: Dependency Audit
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install safety

      - name: Run safety check on dev dependencies
        run: |
          safety check --json || true

      - name: Check for outdated dependencies
        run: |
          pip list --outdated

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Benchmark detection
        run: |
          time python -m ascii_guard.cli lint tests/fixtures/benchmark_test.md

      - name: Benchmark fixing
        run: |
          time python -m ascii_guard.cli fix tests/fixtures/benchmark_test.md --dry-run

  notify-failures:
    name: Notify on Failure
    runs-on: ubuntu-latest
    permissions:
      issues: write
    needs:
      [
        test-multi-platform,
        dependency-audit,
        performance-benchmark,
      ]
    if: failure()

    steps:
      - name: Create issue on failure
        uses: actions/github-script@v8
        with:
          script: |
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ðŸš¨ Scheduled CI Tests Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Scheduled Tests Failed\n\n**Date:** ${new Date().toISOString()}\n\n**Workflow Run:** ${runUrl}\n\nPlease check the workflow run for details and fix any issues.`,
              labels: ['ci', 'scheduled-test-failure']
            });
