name: Scheduled Tests

on:
  schedule:
    # Run every Monday at 00:00 UTC
    - cron: "0 0 * * 1"
  workflow_dispatch:

jobs:
  test-multi-platform:
    name: Test Python 3.12 on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Run tests
        run: |
          pytest -v

      - name: Verify zero dependencies
        run: |
          pytest tests/test_stdlib_only.py -v

  dependency-audit:
    name: Dependency Audit
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install safety

      - name: Run safety check on dev dependencies
        run: |
          safety check --json || true

      - name: Check for outdated dependencies
        run: |
          pip list --outdated

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Create large test file
        run: |
          cat > large_test.md << 'EOF'
          # Large Test File

          This file contains many ASCII boxes for performance testing.
          EOF

          # Create 100 boxes
          for i in $(seq 1 100); do
            cat >> large_test.md << EOF

          ## Box ${i}

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Performance test   â”‚
          â”‚ Box number: ${i}   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          EOF
          done

      - name: Benchmark detection
        run: |
          time python -m ascii_guard.cli lint large_test.md

      - name: Benchmark fixing
        run: |
          time python -m ascii_guard.cli fix large_test.md --dry-run

  notify-failures:
    name: Notify on Failure
    runs-on: ubuntu-latest
    permissions:
      issues: write
    needs:
      [
        test-multi-platform,
        dependency-audit,
        performance-benchmark,
      ]
    if: failure()

    steps:
      - name: Create issue on failure
        uses: actions/github-script@v8
        with:
          script: |
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ğŸš¨ Scheduled CI Tests Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Scheduled Tests Failed\n\n**Date:** ${new Date().toISOString()}\n\n**Workflow Run:** ${runUrl}\n\nPlease check the workflow run for details and fix any issues.`,
              labels: ['ci', 'scheduled-test-failure']
            });
